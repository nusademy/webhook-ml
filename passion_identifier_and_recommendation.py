# -*- coding: utf-8 -*-
"""Passion-identifier-and-Recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VQiiGNxneu42VCTr8kh7v93o9FT1nmpM

# Load the datasets from Kaggle
"""

nfrom google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/Kaggle

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content/drive/My Drive/Kaggle"
!chmod 600 kaggle.json
!ls

!kaggle datasets download -d datasnaek/mbti-type

!unzip \*.zip  && rm *.zip

import numpy as np 
import pandas as pd

data = pd.read_csv("mbti_1.csv")
data.head()

"""# **Spliting the Data into Binary**"""

split_df = data[['type']].copy()

split_df['E-I'] = data['type'].str.extract('(.)[N,S]',1)
split_df['N-S'] = data['type'].str.extract('[E,I](.)[F,T]',1)
split_df['T-F'] = data['type'].str.extract('[N,S](.)[J,P]',1)
split_df['J-P'] = data['type'].str.extract('[F,T](.)',1)

"""Encode Letters to Numeric Value"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

encoded_df = split_df[['type']].copy()
encoded_df['E0-I1'] = le.fit_transform(split_df['E-I'])
encoded_df['N0-S1'] = le.fit_transform(split_df['N-S'])
encoded_df['F0-T1'] = le.fit_transform(split_df['T-F'])
encoded_df['J0-P1'] = le.fit_transform(split_df['J-P'])

binary_type = encoded_df.drop(columns='type')

corrMatrix = binary_type.corr()
corrMatrix.style.background_gradient(cmap='coolwarm')

"""Counting each of type data"""

count_type = data.groupby('type').agg({'type':'count'})
count_type

"""Show the data into graph to more easier read the dataset"""

from matplotlib import pyplot as plt

histogram_chart = count_type.plot(kind='bar')
histogram_chart.set_xlabel("Personality Types")
histogram_chart.set_ylabel("Numbers")
plt.tight_layout()

"""# **Cleaning the Data into Readable Text**"""

http_post = data.copy()
http_post['http_per_post']=data['posts'].apply(lambda x: x.count('http')/50)
http_post.head()

remove_http_post = "(http.*?\s)"
data['no_url'] = data['posts'].replace(remove_http_post,"",regex=True)
data.head()

remove_line = "(\|\|\|)"
data['text']=data['no_url'].replace(remove_line,"",regex=True)
data.head()

remove_punctuations = "[^\w\s]"
data['text']=data['text'].replace(remove_punctuations,"",regex=True)
data.head()

remove_underscore = "\_"
data['text']=data['text'].replace(remove_underscore,"",regex=True)
data.head()

remove_numbers = "\d+"
data['text']=data['text'].replace(remove_numbers,"",regex=True)
data.head()

remove_letter_words = "\W*\b\w\b"
data['text']=data['text'].replace(remove_letter_words,"",regex=True)
data.head()

data['text']=data['text'].str.lower()
data.head()

clean_data = data[['type','text']]
clean_data.head()

"""# **Vectorize Text and Define Features**

Spliting column into 4 combination
"""

split_data = clean_data[['type','text']].copy()
split_data['E-I'] = split_data['type'].str.extract('(.)[N.S]',1)
split_data['N-S'] = split_data['type'].str.extract('[E,I](.)[F,T]',1)
split_data['T-F'] = split_data['type'].str.extract('[N,S](.)[J.P]',1)
split_data['J-P'] = split_data['type'].str.extract('[F,T](.)',1)
split_data.head()

"""Encode letters into numeric"""

from sklearn.preprocessing import LabelEncoder
letters = LabelEncoder()

encoded_data = clean_data[['type','text']].copy()
encoded_data['E0-I1'] =letters.fit_transform(split_data['E-I'])
encoded_data['N0-S1'] =letters.fit_transform(split_data['N-S'])
encoded_data['F0-T1'] =letters.fit_transform(split_data['T-F'])
encoded_data['J0-P1'] =letters.fit_transform(split_data['J-P'])

encoded_data.head()

encoded_data.columns

"""**Vectorize Text**

Define X and Y
"""

x = encoded_data["text"].values
y_all = encoded_data.drop(columns=['type','text'])

from sklearn.model_selection import train_test_split
x_train, x_test, y_all_train, y_all_test = train_test_split(x,y_all,random_state=42)

"""Define TFIDF Vectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    max_features=17000,
    min_df=7,
    max_df=0.8,
    stop_words='english',
    ngram_range=(1,3),
)

import pickle
pickle.dump(vectorizer, open('../Kaggle/vectorizer_indo.pkl','wb'))

"""Vectors for x"""

x_train = vectorizer.fit_transform(x_train)
x_test = vectorizer.transform(x_test)

"""# **Machine Learning Model using Logistic Regression**"""

from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression(solver='lbfgs',random_state=1)
classifier

"""**Extrover(E) - Introvert(I) Type Combination**"""

from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1)

y_EI_train = y_all_train['E0-I1']
y_EI_test = y_all_test['E0-I1']

X_resampled_ros, y_EI_resampled_ros = ros.fit_sample(x_train, y_EI_train)

from collections import Counter
Counter(y_EI_resampled_ros)

"""Fit Introvert-Extrovert Combination with Oversampled (x_train, y_EI_train)"""

classifier.fit(X_resampled_ros, y_EI_resampled_ros)

y_EI_pred_ros = classifier.predict(x_test)
EI_result = pd.DataFrame({"Prediction": y_EI_pred_ros, "Actual": y_EI_test})

import pickle
pickle.dump(classifier, open('../Kaggle/model_EI.pkl','wb'))

"""**N-S Combination**"""

y_NS_train = y_all_train['N0-S1']
y_NS_test = y_all_test['N0-S1']

X_resampled_ros, y_NS_resampled_ros = ros.fit_sample(x_train, y_NS_train)
Counter(y_NS_resampled_ros)

classifier.fit(X_resampled_ros, y_NS_resampled_ros)

y_NS_pred_ros = classifier.predict(x_test)
NS_result = pd.DataFrame({"Prediction": y_NS_pred_ros, "Actual": y_NS_test})
NS_result.head(5)

import pickle
pickle.dump(classifier, open('../Kaggle/model_NS.pkl','wb'))

"""**F-T Combination**"""

y_FT_train = y_all_train['F0-T1']
y_FT_test = y_all_test['F0-T1']

X_resampled_ros, y_FT_resampled_ros = ros.fit_sample(x_train, y_FT_train)
Counter(y_FT_resampled_ros)

classifier.fit(X_resampled_ros, y_FT_resampled_ros)

y_FT_pred_ros = classifier.predict(x_test)
FT_result = pd.DataFrame({"Prediction": y_FT_pred_ros, "Actual":y_FT_test})
FT_result.head()

import pickle
pickle.dump(classifier, open('../Kaggle/model_FT.pkl','wb'))

"""**J-P Combination**"""

y_JP_train = y_all_train['J0-P1']
y_JP_test = y_all_test['J0-P1']

X_resampled_ros, y_JP_resampled_ros = ros.fit_sample(x_train, y_JP_train)
Counter(y_JP_resampled_ros)

classifier.fit(X_resampled_ros, y_JP_resampled_ros)

y_JP_pred_ros = classifier.predict(x_test)
JP_result = pd.DataFrame({"Prediction": y_JP_pred_ros,"Actual": y_JP_test})
JP_result.head()

import pickle
pickle.dump(classifier, open('../Kaggle/model_JP.pkl','wb'))

"""# **Show the Results**"""

from sklearn.metrics import accuracy_score
print(f"E-I:{accuracy_score(y_EI_test, y_EI_pred_ros):.3f}")
print(f"N-S:{accuracy_score(y_NS_test, y_NS_pred_ros):.3f}")
print(f"F-T:{accuracy_score(y_FT_test, y_FT_pred_ros):.3f}")
print(f"J-P:{accuracy_score(y_JP_test, y_JP_pred_ros):.3f}")

"""# **Reference to buld this model**

Datasets that we are using to build this model is MBTI Datasets that open access in Kaggle

Link of the datasets can be access here: https://www.kaggle.com/datasnaek/mbti-type

We also read deck of Davenel Denis, Jing Jin, Steven Walk, and Shaun Wang about Deep MBTI
"""